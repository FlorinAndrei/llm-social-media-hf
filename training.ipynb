{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5382031e",
   "metadata": {},
   "source": [
    "https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261e23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28f84b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'comment_id', 'comment_body', 'parent_text']\n",
      "{\n",
      "  \"timestamp\": \"2021-08-25 16:32:18 UTC\",\n",
      "  \"comment_id\": \"hab6nka\",\n",
      "  \"comment_body\": \"Pfft, just turn the volume way up, it'll blow dry it.\",\n",
      "  \"parent_text\": \"Can confirm, shoulder length hair is not conducive to wearing headphones for the 4 hours after I shower.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"Some system prompt\"\"\"\n",
    "n_example = 4\n",
    "\n",
    "\n",
    "def create_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            # Gemma 3 doesn't support system prompts per se\n",
    "            # {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample['parent_text'],\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": sample['comment_body']},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"csv\", name=\"csv-for-gemma3-hf\", split=\"train\", data_files=\"conversations.csv\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "column_names_orig = dataset.column_names\n",
    "print(column_names_orig)\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28cdbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7fcfcf9c0b4937b8190155cf1ef08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"content\": \"Can confirm, shoulder length hair is not conducive to wearing headphones for the 4 hours after I shower.\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"Pfft, just turn the volume way up, it'll blow dry it.\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(create_conversation, remove_columns=column_names_orig, batched=False)\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb24228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8b9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-3-1b-it-qat-q4_0-unquantized\"\n",
    "# model_id = \"google/gemma-3-4b-it-qat-q4_0-unquantized\"\n",
    "# model_id = \"google/gemma-3-27b-it-qat-q4_0-unquantized\"\n",
    "\n",
    "# Select model class based on id\n",
    "if model_id == \"google/gemma-3-1b-it-qat-q4_0-unquantized\":\n",
    "    model_class = AutoModelForCausalLM\n",
    "else:\n",
    "    model_class = AutoModelForImageTextToText\n",
    "\n",
    "# Check if GPU benefits from bfloat16\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
    "    torch_dtype=torch_dtype, # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\", # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# BitsAndBytesConfig: Enables 4-bit quantization to reduce model size/memory usage\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],\n",
    "    bnb_4bit_quant_storage=model_kwargs['torch_dtype'],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = model_class.from_pretrained(model_id, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load the Instruction Tokenizer to use the official Gemma template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7c957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
